# BookLikeThat #

BookLikeThat is a Python-based book recommendation engine which leverages NLTK's various computational tools to recommend, based off one text the user likes, whether another text may be enjoyable or not. Currently, texts already availble in NLTK's Gutenberg corpus are supported. TF-IDF scores assembled as arrays for each respective text are compared via cosine similarity, forming the backbone of the decision engine.
# Task 1 #

Pipeline 1's purpose is to generate from TFRecord-encoded BidLog proto messages three separate PCollections: DeviceProfiles, AppProfiles, and SuspiciousIDs. BidLogs are log data from ad-bidding marketplaces. BidLogs contain useful information, such as the original bid request, the bid price, the bid result, the exchange at which the bid was placed, as well as timestamps when the bid was recieved and processed. These BidLogs are used to generate DeviceProfiles, which contain IDs for the given device, applications to which the device is subscribed, locations at which the device was used, as well as timestamps of first and last device activity. We develop from these DeviceProfiles through various PTransformations AppProfiles and SuspiciousIDs: using aggregated information on various applications listed as used in DeviceProfiles, we generate AppProfiles, which contains data related to particular, unique, applications. Another PTransform uses these AppProfiles and DeviceProfiles to determine whether a given DeviceID may be suspicious, meaning that by certain characteristics of the relevant DeviceID's data, we believe the device may be being utilized by a non-human. See the comments for these various PCollections and transformations under BidLogJob for more specific information.

# Pipeline 2 #
Pipeline 2's purpose is to generate a PCollection of DeviceProfiles which, after filtering out SuspiciousIDs, will be fed into a TensorFlow model to predict the likelihood of a given device making another purchase within the next day or so. The results of this modeling data are uploaded to Google Cloud Storage in JSON format and also to Google BigQuery to be queryable. See the comments for these various PCollections and transformations under PredictionJob for more specific information.

